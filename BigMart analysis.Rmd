---
title: "Big market analysis"
author: '1'
date: "1 June 2018"
output: html_document
---

### Problem Statement
The data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined. The aim is to build a predictive model and find out the sales of each product at a particular store.

Using this model, BigMart will try to understand the properties of products and stores which play a key role in increasing sales

The data may have missing values as some stores might not report all the data due to technical glitches. Hence, it will be required to treat them accordingly.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(data.table) # used for reading and manipulation of data
library(dplyr)      # used for data manipulation and joining
library(ggplot2)    # used for ploting 
library(caret)      # used for modeling
library(corrplot)   # used for making correlation plot
#library(xgboost)    # used for building XGBoost model
library(gridExtra) 
library(plotly)
library(glmnet)
setwd("E://downloads//analytical vidhya")
train = fread("Train_UWu5bXk.csv",na.strings = "")
test = fread("Test_u94Q5KV.csv",na.strings = "")
submission = fread("SampleSubmission_TmnO39y.csv")

#View(train)
#View(test)
#View(submission)
```

### Hypothesis generation
On what factors sales are depended according to that frame hypothesis


### Dimensions of dataset
```{r}
dim(train);dim(test)

```


### combining test and train dataset for performaing EDA and exploring the dataset
```{r}
test[,Item_Outlet_Sales := NA]
combid = rbind(train, test)
dim(combid)

```

```{r}
str(combid)
```

### Missing values
```{r}
sapply(combid, function(x){
  sum(is.na(x))
})

```
### Imputing missing values
```{r}
missing_index = which(is.na(combid$Item_Weight))

for (i in missing_index){
  item = combid$Item_Identifier[i]
  combid$Item_Weight[i]=mean(combid$Item_Weight[combid$Item_Identifier == item],na.rm=TRUE)
}
```

- no missing values in the item_weight column
```{r}
sum(is.na(combid$Item_Weight))

```


### Replacing zero's in Item_visibility
```{r}
zero_index=which(combid$Item_Visibility==0)

for(i in zero_index){
  item=combid$Item_Identifier[i]
  combid$Item_Visibility[i]=mean(combid$Item_Visibility[combid$Item_Identifier==item],na.rm = TRUE)
}

```

```{r}
sum(combid$Item_Visibility==0)
```


### univariate analysis

- Target variable
- Right skewd need to perform data transformation to treat skewness
```{r}

ggplot(train)+geom_histogram(aes(train$Item_Outlet_Sales),binwidth = 100,fill="red")+xlab("Item_Outlet_Sales")


```

- Independent continous variables
- No specific pattren can be seen from Item_Weight plot
- Item_Visibility plot is right skwe
- There are 4 different distributions for Item_MRP 
```{r}

p1=ggplot(combid)+geom_histogram(aes(x=Item_Weight),binwidth =0.5,fill="darkgreen")+xlab("Item_Weight")
p2=ggplot(combid)+geom_histogram(aes(x=Item_Visibility),binwidth = 0.005,fill="blue")+xlab("Item_Visibility")
p3=ggplot(combid)+geom_histogram(aes(x=Item_MRP),binwidth = 1,fill="orange")+xlab("Item_MRP")

grid.arrange(p1,p2,p3)

```

### Independent variable (categorical variable)
- Data cleansing: LF to low fat and reg to Regular
- The sales of Low Fat products are comparitively higher than regular products
```{r}
combid$Item_Fat_Content[combid$Item_Fat_Content=="LF"]<-"Low Fat"
combid$Item_Fat_Content[combid$Item_Fat_Content=="low fat"]<-"Low Fat"
combid$Item_Fat_Content[combid$Item_Fat_Content=="reg"]<-"Regular"

ggplot(combid%>%group_by(Item_Fat_Content)%>%summarise(count=n()))+geom_bar(aes(Item_Fat_Content,count),stat = "identity",fill="coral1")
```

- Outlet_type
- Supermarket 1 seems to be more popular out of other markets
```{r} 
ggplot(combid%>%group_by(Outlet_Type)%>%summarise(count=n()))+geom_bar(aes(Outlet_Type,count),stat="identity",fill="purple")
```

### Imputing missing values in the column Outlet_Size
```{r}
## The following Outlet_Identifier below need to be imputed
combid%>%filter(is.na(Outlet_Size))%>%select(Outlet_Identifier)%>%unique()

## kind of store type for this 3 identiy store is supermarket1 and grocery market
combid%>%filter(Outlet_Identifier %in% c("OUT017","OUT045","OUT010"))



## In tier 2 there are three stores "OUT017","OUT045","OUT035" out of which one is small so the other two can also be considered as small
combid%>%filter(Outlet_Identifier %in% c("OUT017","OUT045")&Outlet_Location_Type=="Tier 2")

## The OUT010 and OUT019 is a grocery market present in tier 1 and tier3 out of which  OUT019 is small so the OUT010 can also be imputed as small
combid%>%filter(Outlet_Identifier=="OUT010"& Outlet_Location_Type %in% c("Tier 1","Tier 3"))

## Imputing missing values in column Outlet_Size with small

combid$Outlet_Size[is.na(combid$Outlet_Size)]<-"Small"
sum(is.na(combid$Outlet_Size))

```

- plot of Outlet_Size
```{r}
ggplot(combid%>%group_by(Outlet_Size)%>%summarise(count=n()))+geom_bar(aes(Outlet_Size,count),stat="identity",fill="pink")+theme_bw()

```

- Plot for Outlet_Establishment_Year
- lesser number of observations for the data in the year 1998
```{r}
ggplot(combid %>% group_by(Outlet_Establishment_Year) %>% summarise(Count = n())) + 
  geom_bar(aes(factor(Outlet_Establishment_Year), Count), stat = "identity", fill = "coral1") +
  geom_label(aes(factor(Outlet_Establishment_Year), Count, label = Count), vjust = 0.5) +
  xlab("Outlet_Establishment_Year") +
  theme(axis.text.x = element_text(size = 8.5))
```

### Target Variable vs Independent Categorical Variables
- How are item sales across different categorical variables 
- Item sales for different types of products 
- The distribution of sales across item_fat_content is same 

```{r message=FALSE, warning=FALSE}
ggplot(combid) + 
      geom_violin(aes(Item_Fat_Content, Item_Outlet_Sales), fill = "magenta") +
      theme(axis.text.x = element_text(angle = 45, hjust = 1),
            axis.text = element_text(size = 8),
            axis.title = element_text(size = 8.5))
```


- Tier 1 and Tier 3 locations of Outlet_Location_Type look similar.
- In the Outlet_Type plot, Grocery Store has most of its data points around the lower sales values as compared to the other categories.

```{r}
p1 = ggplot(train) + geom_violin(aes(Outlet_Location_Type, Item_Outlet_Sales), fill = "magenta")
p2 = ggplot(train) + geom_violin(aes(Outlet_Type, Item_Outlet_Sales), fill = "magenta")
grid.arrange(p1,p2)
```

### Target Variable vs Independent Continous Variables
- Item_MRP vs Item_Outlet_Sales
- we can clearly see 4 segments of prices

```{r}

plo1=ggplot(train) + 
      geom_point(aes(Item_MRP, Item_Outlet_Sales), colour = "steel blue", alpha = 0.3) +
      theme(axis.title = element_text(size = 8.5))+theme_bw()
ggplotly(plo1)


```



### Feature Engineering
```{r}
unique(combid$Item_Type)
```

- Categorising the above list of products into perishable and non-perishable 
```{r}
 perishable=c("Fruits and Vegetables", "Breads", "Dairy","Meat","Seafood", "Breakfast")
non_perishable=c("Soft Drinks", "Baking Goods","Hard Drinks", "Canned", "Starchy Foods","Household", "Frozen Foods")

## new_feature column Item_Type_new

combid[,Item_Type_new:=ifelse(Item_Type %in% perishable,"perishable",ifelse(Item_Type %in% non_perishable,"non_perishable","not sure"))]

colnames(combid)

```

- Item_Identifier 
- The first two characters in Item_Identifier are "FD"(food),"NC"(non-consumable),"DR"(drinks)
- comparing item_type with the Item_Identifier
```{r}
table(combid$Item_Type,substr(combid$Item_Identifier,1,2))

```

- based on above table we can create a new column naming item_category
- replacing Item_fat content with non-edible wherever it is NC
```{r}
combid[,Item_category:=substr(combid$Item_Identifier,1,2)]
combid$Item_Fat_Content[combid$Item_category == "NC"] = "Non-Edible"


combid
```

- Price per unit weight

```{r}

combid[,price_per_unit_wt := Item_MRP/Item_Weight]

ggplot(combid)+geom_histogram(aes(price_per_unit_wt),binwidth = 1,fill="maroon")


```

- Item_MRP was speard into 4 different segments as we saw in the plot above b/w Item_MRP and sales
- Item_MRP can be categorized into 4 different segments "low price","High price","medium price 1","medium price 2"
```{r}
combid[,Item_price_group:=ifelse(combid$Item_MRP<70,"low price",
                          ifelse(combid$Item_MRP>=70 & combid$Item_MRP < 125,"medium price_1",
                          ifelse(combid$Item_MRP >= 125 & combid$Item_MRP < 270, "medium price_2", "High price")))]

```

```{r}
str(combid)
```

- converting catergorical columns to numerical columns
```{r}
hot_code=dummyVars("~.",data = combid[,-c("Item_Identifier","Item_Type","Outlet_Establishment_Year")],fullRank = T)

hotc_df = data.table(predict(hot_code, combid[,-c("Item_Identifier", "Outlet_Establishment_Year", "Item_Type")]))
combid = cbind(combid[,c("Item_Identifier")], hotc_df)


```


### Data preprocessing
- Removing Skewness 
- Skewness in variables is undesirable for predictive modeling. Some machine learning methods assume normally distributed data and a skewed variable can be transformed by taking its log, square root, or cube root so as to make its distribution as close to normal distribution as possible. In our data, variables Item_Visibility and price_per_unit_wt are highly skewed. So, we will treat their skewness with the help of log transformation.
```{r}
combid[,Item_Visibility := log(Item_Visibility + 1)] # log + 1 to avoid division by zero
combid[,price_per_unit_wt := log(price_per_unit_wt + 1)]

```

- Scaling numeric predictors
- scale and center the numeric variables to make them have a mean of zero, standard deviation of one and scale of 0 to 1. Scaling and centering is required for linear regression models

```{r}
num_vars = which(sapply(combid, is.numeric)) # index of numeric features
num_vars_names = names(num_vars)
combid_numeric = combid[,setdiff(num_vars_names, "Item_Outlet_Sales"), with = F]
prep_num = preProcess(combid_numeric, method=c("center", "scale"))
combid_numeric_norm = predict(prep_num, combid_numeric)

```


```{r}
combid[,setdiff(num_vars_names, "Item_Outlet_Sales") := NULL] # removing numeric independent variables
combid = cbind(combid, combid_numeric_norm)
```


- splitting combined data into train and test
```{r}

train = combid[1:nrow(train)]
test = combid[(nrow(train) + 1):nrow(combid)]
test[,Item_Outlet_Sales := NULL] # removing Item_Outlet_Sales as it contains only NA for test dataset


```

### checking correlation
- It is not desirable to have correlated features if we are using linear regressions.
- Variables price_per_unit_wt and Item_Weight are highly correlated as the former one was created from the latter. Similarly price_per_unit_wt and Item_MRP are highly correlated for the same reason.
```{r}
cor_train = cor(train[,-c("Item_Identifier")])
corrplot(cor_train, type = "lower", tl.cex = 0.9)
```
### Linear regression
- Multicollinearity: This phenomenon exists when the independent variables are found to be moderately or highly correlated. In a model with correlated variables, it becomes a tough task to figure out the true relationship of a predictors with response variable. In other words, it becomes difficult to find out which variable is actually contributing to predict the response variable
-  The presence of non-constant variance in the error terms results in heteroskedasticity. 
-  5-fold  CV is used here it basically  gives an idea as to how well a model generalizes to unseen data.
```{r}
linear_reg_mod = lm(Item_Outlet_Sales ~ ., data = train[,-c("Item_Identifier")])

```

- making prediction on test data
- got an RMSE of 1202.33 on the public leaderboard, but this score has been calculated by using only the 25% (public) of the test data and we have no idea how this model will perform on the other 75% (private) of the test data. So, there has to be a system in place for us to check generalizability of our model, in other words, how consistently our model performs at unseen data or new data

```{r}
# preparing dataframe for submission and writing it in a csv file
submission$Item_Outlet_Sales = predict(linear_reg_mod, test[,-c("Item_Identifier")])
write.csv(submission, "Linear_Reg_submit1.csv", row.names = F)



```

-
### lasso 
-Regularised regression models can handle the correlated independent variables well and helps in overcoming overfitting. Ridge penalty shrinks the coefficients of correlated predictors towards each other, while the Lasso tends to pick one of a pair of correlated features and discard the other. The tuning parameter lambda controls the strength of the penalty.
-  RMSE: 1129.844

```{r}
set.seed(1235)
my_control = trainControl(method="cv", number=5)
Grid = expand.grid(alpha = 1, lambda = seq(0.001,0.1,by = 0.0002))

lasso_linear_reg_mod = train(x = train[, -c("Item_Identifier", "Item_Outlet_Sales")], y = train$Item_Outlet_Sales,
                       method='glmnet', trControl= my_control, tuneGrid = Grid)
lasso_linear_reg_mod$results[1,3]

```


### ridge
- RMSE: 1134.188 

```{r}
set.seed(1236)
my_control = trainControl(method="cv", number=5)
Grid = expand.grid(alpha = 0, lambda = seq(0.001,0.1,by = 0.0002))

ridge_linear_reg_mod = train(x = train[, -c("Item_Identifier", "Item_Outlet_Sales")], y = train$Item_Outlet_Sales,
                       method='glmnet', trControl= my_control, tuneGrid = Grid)
ridge_linear_reg_mod[[4]][1,3]
```


RandomForest is a tree based bootstrapping algorithm wherein a certain number of weak learners (decision trees) are combined to make a powerful prediction model. For every individual learner, a random sample of rows and a few randomly chosen variables are used to build a decision tree model. Final prediction can be a function of all the predictions made by the individual learners. In case of a regression problem, the final prediction can be mean of all the predictions

We will now build a RandomForest model with 400 trees. The other tuning parameters used here are mtry - no. of predictor variables randomly sampled at each split, and min.node.size - minimum size of terminal nodes (setting this number large causes smaller trees and reduces overfitting).


```{r}
library(ranger)
library(caret)
set.seed(1237)
my_control = trainControl(method="cv", number=5) # 5-fold CV
tgrid = expand.grid(
  .mtry = c(3:10),
  .splitrule = "variance",
  .min.node.size = c(10,15,20)
)
rf_mod = train(x = train[, -c("Item_Identifier", "Item_Outlet_Sales")], 
               y = train$Item_Outlet_Sales,
               method='ranger', 
               trControl= my_control, 
               tuneGrid = tgrid,
               num.trees = 400,
               importance = "permutation")

rf_mod$results


```

As per the plot shown above, the best score is achieved at mtry = 5 and min.node.size = 20.

```{r}
plot(rf_mod)
```


### Variable Importance
As expected Item_MRP is the most important variable in predicting the target variable. New features created by us, like price_per_unit_wt, Outlet_Years, Item_MRP_Clusters, are also among the top most important variables.
```{r}
plot(varImp(rf_mod))
```

After trying and testing 5 different algorithms, the best score on the public leaderboard has been achieved by XGBoost (1154.70), followed by RandomForest (1157.25). However, there are still a plenty of things that we can try to further improve our predictions.4



